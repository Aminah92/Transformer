# Transformer: ATTENTION IS ALL YOU NEED
This is an implementation from scratch of the original paper "Attention is all you need"




![architecture](The-transformer-model-from-Attention-is-all-you-need-Viswani-et-al.png)

Paper Reference: https://arxiv.org/abs/1706.03762



Major modules implemented in the code

- Positional Encoder
- Scaled Dot Product Attention
- Multihead Attention
- Layer Norm/Residual Connection
- Encoder
- Decoder
- Masking(Target and Source)
