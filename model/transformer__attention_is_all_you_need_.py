# -*- coding: utf-8 -*-
"""Transformer-"Attention is all you need".ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1POGErpRsNAxDy1bqLJL6kBUb1kni4CXl
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as f

"""# Encoder - Deocder Stack

## 1. Encoder Block
- composed of a stack of N = 6 identical layers.
- Each Layer has 2 sub-layers


1.   the Feed Forward Layer
2.   the Self-Attention Layer

The output from the Self-Attention is fed as input to the Feed-Forward Network

# 1. Embeddings and Softmax
- Using the learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.

- In the embedding layer, the weights are multiplied by √dmodel.
"""

# The Input Embeddings:

class Embeddings(nn.module):
  def __init__(self, vocab_size, dmodel):
    """ 
    Vocab_Size = Length os entire sequence fed into the network
    """
      super(Embeddings, self).__init__()
      self.embed = nn.Embedding(vocab_size, dmodel) 
      self.dmodel = dmodel
  def forward(self, x):
      return self.embed(x) * math.sqrt(self.dmodel)

def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

class Encoder(nn.Module):
    "Core encoder is a stack of N = 6 layers"
    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.embed = Embeddings(vocab_size, dmodel)
        self.pe = PositionalEncoder(dmodel)
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, mask):
        "Pass the input (and optional  mask) through each layer in turn."
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)





class MultiheadAttention(nn.module):
  
  def __init__(self,dmodel,h):
    """ Where the projections are parameter matrices 
    W^Qi ∈ R^(dmodel×dk) , 
    W^Ki ∈ R^(dmodel×dk) , 
    W^Vi ∈ R^(dmodel×dv)
    W^O ∈ R^(hdv×dmodel)
    dk=dv=dq= dmodel/h
    """
    super(MultiheadAttention, self).__init__()
    self.dmodel = dmodel #512 or 256
    self.h = h #8 layers
    self.dk = dmodel // h #

    assert dk * h == dmodel "division between embed dimensions and stack layers need to be an integer"

    #3 linear layers for Q, V and K + one for the ouput Layer fc_out

    self.wq = nn.Linear(dmodel,dmodel) 
    self.wk = nn.Linear(dmodel,dmodel)
    self.wv = nn.Linear(dmodel,dmodel)
    self.fc_out =  nn.Linear(dmodel,dmodel)
    #self.dropout = nn.Dropout(p=0.2)
  def forward(self, q,k,v,mask=None):
    """
    Args: 
    q = Query
    k = Key
    v = Value
    bs = batch size
    sl = sequence length
    """
    if mask != None:
      mask = mask.unsqueeze(1)
    bs = q.size(0) #batch size

    #Splitting into heads for Linear operation
    
    
    q = self.wq(q).view(bs, -1, self.h, self.dk)
    k = self.wk(k).view(bs, -1, self.h, self.dk)
    v = self.wv(v).view(bs, -1, self.h, self.dk)
        
    # transpose to get dimensions bs * h * sl * dmodel
       
    k = k.transpose(1,2)
    q = q.transpose(1,2)
    v = v.transpose(1,2)
    #Scaled Dot Product attention for projected versions of queries, keys and values
    score = attention(q, k, v, self.dk, mask, self.dropout)
        
    # concatenate heads 
    concat = scores.transpose(1,2).contiguous()\
    .view(bs, -1, self.dmodel)
    
    output = self.out(concat)

    return output

#SCaled Dot Product Attention
def attention(q, k, v, dk,  dropout=None, mask=None):
    """' Compute 'Scaled Dot Product Attention'
    q-query: bs, n, dmodel
    k-key: bs, n, dmodel
    v-value: bs, n, dmodel
    att_scores = Attention
    Mask optional: As mentioned in the Paper
    Attention(Q, K, V ) = softmax(Q*K.T/√dk)*V
    
    """
    mat_mult = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) #matmul + scaling
    #Optional Mask 
    # if mask != None:
    #     mask = mask.unsqueeze(1)
    #     att_scores = scores.masked_fill(mask == 0, -1e9)
    
    #Softmax Computation
    scores = f.softmax(scores, dim = -1) #softmax

    #Optional Dropout Implementation
    if dropout != None:
        scores = dropout(scores)           
    output = torch.matmul(scores, v)    #final matmul with v
    return output



"""# Positional Encoding"""

class PositionalEncoder(nn.Module):
    """Implement the PE function.

    PE(pos,2i) = sin(pos/100002i/dmodel)
    PE(pos,2i+1) = cos(pos/100002i/dmodel)
    dim_PE == dmodel
    pos: position of words
    i: dimension
    
    """ 
    class PositionalEncoder(nn.Module):
    "Implement the PE function."
    def __init__(self, dmodel, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=0.1)
        
        # Compute the positional encodings
        pe = torch.zeros(max_len, dmodel)
        pos = torch.arange(0, max_len).unsqueeze(1)
        div_exp_term = torch.exp(torch.arange(0, dmodel, 2) *
                             -(math.log(10000.0) / dmodel))
        pe[:, 0::2] = torch.sin(pos * div_exp_term)
        pe[:, 1::2] = torch.cos(pos * div_exp_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], 
                         requires_grad=False)
        return self.dropout(x)