"""Transformer-"Attention is all you need".ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1POGErpRsNAxDy1bqLJL6kBUb1kni4CXl
"""

import math
import argparse
import time
import copy
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as f
from torch.nn import Linear, ReLU, Dropout



!nvidia-smi

def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !unzip fr-en.tgz

en = "/content/english.txt"

#Config
dmodel = 512
h = 8
N = 6
en = "/content/english.txt"
fr = "/content/french.txt"

src_vocab = len(en)
trg_vocab = len(fr)



lr = 0.0001 #random selection
b1 = 0.9
b2 = 0.98
eps = 1e-9


"""

# The Input Embeddings:

class Embeddings(nn.Module):
  def __init__(self, vocab_size, dmodel):
    """ 
    Vocab_Size = Length os entire sequence fed into the network
    """
    super(Embeddings, self).__init__()
    self.embed = nn.Embedding(vocab_size, dmodel) 
    self.dmodel = dmodel
  def forward(self, x):
      return self.embed(x) * math.sqrt(self.dmodel)

class PositionalEncoder(nn.Module):
    """Implement the PE function.

    PE(pos,2i) = sin(pos/100002i/dmodel)
    PE(pos,2i+1) = cos(pos/100002i/dmodel)
    dim_PE == dmodel
    pos: position of words
    i: dimension
    
    """ 
    
    
    def __init__(self, dmodel, max_len=5000):
      "Implement the PE function."
      super(PositionalEncoder, self).__init__()
      self.dropout = nn.Dropout(p=0.1)
      
      # Compute the positional encodings
      pe = torch.zeros(max_len, dmodel)
      pos = torch.arange(0, max_len).unsqueeze(1)
      div_exp_term = torch.exp(torch.arange(0, dmodel, 2) *
                            -(math.log(10000.0) / dmodel))
      pe[:, 0::2] = torch.sin(pos * div_exp_term)
      pe[:, 1::2] = torch.cos(pos * div_exp_term)
      pe = pe.unsqueeze(0)
      self.register_buffer('pe', pe)
        
    def forward(self, x):
      x = x + Variable(self.pe[:, :x.size(1)], 
                        requires_grad=False)
      return self.dropout(x)

# #
# class PFFN(nn.Module):
#   "Implements the Positive Feed Forward Network"
#   def __init__(self,dmodel):
#     super(PFFN, self).__init__()
    
#     model =  nn.Sequential(
#             nn.Linear(dmodel, 2048),
#             nn.ReLU(),
#             nn.Linear(2048, dmodel),
#             nn.Dropout(p=0.1)
#         )
#     return model

class PFFN(nn.Module):
  def __init__(self,dmodel):
    super(PFFN, self).__init__()
    self.f1 = nn.Linear(dmodel, 2048)
    self.dropout = nn.Dropout(p=0.1)
    self.f2 = nn.Linear(2048, dmodel)
    self.relu = nn.ReLU
  def forward(self, x):
    x = self.dropout(self.relu(self.f1(x)))
    x = self.f2(x)
    return x

#SCaled Dot Product Attention
def attention(q, k, v, dk,  dropout=None, mask=None):
    """' Compute 'Scaled Dot Product Attention'
    q-query: bs, n, dmodel
    k-key: bs, n, dmodel
    v-value: bs, n, dmodel
    att_scores = Attention
    Mask optional: As mentioned in the Paper
    Attention(Q, K, V ) = softmax(Q*K.T/√dk)*V
    
    """
    mat_mult = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) #matmul + scaling
    Optional Mask 
    if mask != None:
        mask = mask.unsqueeze(1)
        att_scores = scores.masked_fill(mask == 0, -1e9)
    
    #Softmax Computation
    scores = f.softmax(scores, dim = -1) #softmax

    #Optional Dropout Implementation
    if dropout != None:
        scores = dropout(scores)           
    output = torch.matmul(scores, v)    #final matmul with v
    return output

class MultiheadAttention(nn.Module):
  
  def __init__(self,dmodel,h):
    """ Where the projections are parameter matrices 
    W^Qi ∈ R^(dmodel×dk) , 
    W^Ki ∈ R^(dmodel×dk) , 
    W^Vi ∈ R^(dmodel×dv)
    W^O ∈ R^(hdv×dmodel)
    dk=dv=dq= dmodel/h
    """
    super(MultiheadAttention, self).__init__()
    self.dmodel = dmodel #512 or 256
    self.h = h #8 layers
    self.dk = dmodel // h #
   # print(h)
   # print(dmodel)

    #assert self.dk * self.h == dmodel, "division between embed dimensions and stack layers need to be an integer"

    #3 linear layers for Q, V and K + one for the ouput Layer fc_out

    self.wq = nn.Linear(dmodel,dmodel) 
    self.wk = nn.Linear(dmodel,dmodel)
    self.wv = nn.Linear(dmodel,dmodel)
    self.fc_out =  nn.Linear(dmodel,dmodel)
    #self.dropout = nn.Dropout(p=0.2)
  def forward(self, q,k,v,mask=None):
    """
    Args: 
    q = Query
    k = Key
    v = Value
    bs = batch size
    sl = sequence length
    """
    if mask != None:
      mask = mask.unsqueeze(1)
    bs = q.size(0) #batch size

    #Splitting into heads for Linear operation
    
    
    q = self.wq(q).view(bs, -1, self.h, self.dk)
    k = self.wk(k).view(bs, -1, self.h, self.dk)
    v = self.wv(v).view(bs, -1, self.h, self.dk)
        
    # transpose to get dimensions bs * h * sl * dmodel
       
    k = k.transpose(1,2)
    q = q.transpose(1,2)
    v = v.transpose(1,2)
    #Scaled Dot Product attention for projected versions of queries, keys and values
    score = attention(q, k, v, self.dk, mask, self.dropout)
        
    # concatenate heads 
    concat = scores.transpose(1,2).contiguous().view(bs, -1, self.dmodel)
    
    output = self.out(concat)

    return output

class LayerNorm(nn.Module):
    "Implement a layernorm."
    def __init__(self, dmodel, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.alpha = nn.Parameter(torch.ones(dmodel))
        self.beta = nn.Parameter(torch.zeros(dmodel))
        self.eps = eps

    def forward(self, x):
        mu = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.alpha * (x - mu) / (std + self.eps) + self.beta





#class ResidualLayer(nn.Module): 
#  "Implements the residual connection for the sublayers"
#  def __init__(self, dmodel):
#      super(ResidualLayer, self).__init__()
#      self.norm = LayerNorm(dmodel)
#      self.dropout = nn.Dropout(0.1)

#  def forward(self, x, sublayer):
#      return x + self.dropout(sublayer(self.norm(x)))

class Encoder(nn.Module):
    "Core encoder is a stack of N = 6 layers"
    def __init__(self, vocab_size,dmodel,h, N=6):
        super(Encoder, self).__init__()
        self.layers = clones(EncoderLayer(dmodel, h),N) #Cloned layers N times 
        self.embed = Embeddings(vocab_size, dmodel) #The embeding class
        self.pe = PositionalEncoder(dmodel) #The positional Encoder
        self.norm = LayerNorm(dmodel) #Add + Normalize over each layer x + z
        self.N = N
    def forward(self, x,mask):
        "Pass the input through each layer in turn.Mask is optional in Encoder Layer"
        for layer in self.layers:
            x = layer(x,mask)
        return self.norm(x)

class EncoderLayer(nn.Module):
    "Implements the encoder with the two key features(Feed forward and Attention)"
    def __init__(self, dmodel,h):
        super(EncoderLayer, self).__init__()
        self.attention = MultiheadAttention(dmodel,h)
        self.pffn = PFFN(dmodel)
        self.sublayer1 = LayerNorm(dmodel)
        self.sublayer2 = LayerNorm(dmodel)
        self.dropout = nn.Dropout(p=0.1)

    def forward(self, x, mask):
      xnew = self.sublayer1(x)
      x = x + self.dropout(self.attention(xnew,xnew,xnew,mask))
      xnew = self.sublayer2(x)
      x = x + self.dropout(self.pffn(xnew))
      return x

class Decoder(nn.Module):
    "Generic N layer decoder with masking."
    def __init__(self, vocab_size,dmodel,h, N):
        super(Decoder, self).__init__()
        self.N = N
        self.embed = Embeddings(vocab_size,dmodel)
        self.pe = PositionalEncoder(dmodel)
        self.layer = DecoderLayer(vocab_size,dmodel)
        self.layers = clones(self.layer, N)
        self.norm = LayerNorm(dmodel)
        
    def forward(self, trg, encoder_out, src_mask, trg_mask):
        x = self.embed(trg)
        x = self.pe(x)
        for i in range(self.N):
            x = self.layers[i](x, encoder_out, src_mask, trg_mask)
        return self.norm(x)



class DecoderLayer(nn.Module):
    "Implements the decoder L"
    def __init__(self, dmodel,h):
        super(DecoderLayer, self).__init__()
        self.attention = MultiheadAttention(dmodel,h)
        self.pffn = PFFN(dmodel)
        self.sublayer1 = LayerNorm(dmodel)
        self.sublayer2 = LayerNorm(dmodel)
        self.sublayer3 = LayerNorm(dmodel)
        self.dropout = nn.Dropout(p=0.1)
    def forward(self, x, encoder_out, src_mask, trg_mask): 
      "Takes Encoder output in the second layer and a source and target mask"
      xnew = self.sublayer1(x)
      x = x + self.dropout(self.attention(xnew,xnew,xnew,trg_mask))
      xnew = self.sublayer2(x)
      x = x + self.dropout(self.attention(xnew,encoder_out,encoder_out,src_mask))
      xnew = self.sublayer3(x)
      x = x + self.dropout(self.pffn(xnew))
      return x

class Transformer(nn.Module):
    def __init__(self, src, tgt, dmodel, h,N):
        super().__init__()
        self.encoder = Encoder(src_vocab, dmodel, h, N)
        self.decoder = Decoder(trg_vocab, dmodel, h, N)
        self.out = nn.Linear(dmodel, trg_vocab)
    def forward(self, src, trg, src_mask, trg_mask):
        encoder_outputs = self.encoder(src, src_mask)
        decoder_output = self.decoder(trg, encoder_outputs, src_mask, trg_mask)
        output = self.out(decoder_output)
        return output

model = Transformer(src_vocab, trg_vocab, dmodel, h,N)

512//8

for p in model.parameters():
    if p.dim() > 1:
        nn.init.xavier_uniform_(p)

optim = torch.optim.Adam(model.parameters(), lr=lr, betas=(b1, b2), eps=eps)
criterion = nn.CrossEntropy()
